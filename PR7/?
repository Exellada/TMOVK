1. Подготовка
Устанавливаются библиотеки:

tensorflow — для работы с нейросетями.

adversarial-robustness-toolbox (ART) — для защиты от атак.

Загружается датасет MNIST (изображения цифр 28x28 пикселей).

2. Создание нейросети
Строится простая свёрточная нейросеть (CNN) для классификации цифр:

Свёрточные слои (выделяют признаки, например, линии, круги).

Max-pooling (уменьшает размерность картинки).

Полносвязные слои (анализируют признаки и принимают решение).

Обучение происходит через минимизацию ошибки (кросс-энтропия) с оптимизатором Adam.

3. Атака на нейросеть
Fast Gradient Method (FGSM) — метод "взлома" нейросети:

Берётся обычная картинка (например, цифра "3").

К ней добавляются незаметные человеческому глазу искажения.

Нейросеть ошибается (например, видит "5" вместо "3").

После атаки точность модели падает (например, с 98% до 40%).

4. Защита через Adversarial Training
Adversarial Trainer дообучает нейросеть на "взломанных" данных:

Модель видит как обычные данные, так и атакованные.

Учится распознавать и те, и другие корректно.

После защиты:

Точность на атакованных данных повышается (например, с 40% до 85%).

Точность на обычных данных остаётся высокой.

Аналогия с человеком
Представьте, что нейросеть — это ученик, который учится различать цифры:

Обычное обучение:

Учитель показывает картинки цифр.

Ученик запоминает и хорошо сдаёт тест.

Атака FGSM:

Хакер рисует цифры с почти незаметными искажениями.

Ученик ошибается, потому что не видел таких примеров раньше.

Adversarial Training:

Учитель теперь показывает и обычные цифры, и "обманки".

Ученик учится распознавать оба типа и становится устойчивее к обману.

Итог
Код показывает:

Как нейросети можно "ломать" незаметными изменениями в данных.

Как защитить модель, тренируя её на таких "взломанных" примерах.


